from ctypes import c_int64
import gzip
from multiprocessing import Process, Queue
from multiprocessing.sharedctypes import RawArray, Value
import os
from datetime import datetime
from math import log10

import numpy

from .fasta import readFastaFile, getNx
from .fastani import runFastani
from .mash import runMash
from .processbar import ProcessBar


def isGzipped(file):
    openFile = open(file, 'rb')
    magicCode = openFile.read(2)
    openFile.close()
    return magicCode == b'\x1f\x8b'


def readCheckm2File(file, contamination, completeness):
    contamination *= 100
    completeness *= 100
    cluster2completenessContamination = dict()
    if isGzipped(file):
        openFile = gzip.open(file, mode = 'rt')
    else:
        openFile = open(file, 'r')
    assert openFile.readline().startswith('Name\tCompleteness\tContamination'), f'\"{file}\" is not a valid output generated by CheckM2.'
    for line in openFile:
        lines = line.rstrip('\n').split('\t')
        completeness_ = float(lines[1])
        contamination_ = float(lines[2])
        if completeness_ >= completeness and contamination_ <= contamination:
            cluster2completenessContamination[lines[0]] = (float(lines[1]), float(lines[2]))
    openFile.close()
    return cluster2completenessContamination


def readGtdbtkFile(file):
    cluster2dpcofg_s = dict()
    if isGzipped(file):
        openFile = gzip.open(file, mode = 'rt')
    else:
        openFile = open(file, 'r')
    assert openFile.readline().startswith('user_genome\tclassification'), f'\"{file}\" is not a valid output generated by GTDB-Tk.'
    for line in openFile:
        lines = line.rstrip('\n').split('\t', maxsplit = 2)
        dpcofg, s = lines[1].rsplit(';', maxsplit = 1)
        if not dpcofg.endswith('g__'):
            cluster2dpcofg_s[lines[0]] = (dpcofg, s)
    openFile.close()
    return cluster2dpcofg_s


def createMapping(files):
    cluster2file = dict()
    for i in files:
        cluster2file[os.path.splitext(os.path.basename(i))[0]] = i
    return cluster2file


def computeN50Worker(queue, x, n, N):
    processBar = ProcessBar(N)
    while True:
        i, file = queue.get()
        if i is None:
            break
        x[i] = getNx(file, 50)
        n.acquire()
        n.value += 1
        processBar.plot(n.value)
        n.release()
    return None


def computeN50(clusters, cluster2file, threads):
    cluster2n50 = dict()
    n50s = RawArray(c_int64, len(clusters))
    N = len(clusters)
    n = Value(c_int64, 0)
    queue = Queue(threads)
    processes = list()
    for i in range(threads):
        processes.append(Process(target = computeN50Worker, args = (queue, n50s, n, N)))
        processes[-1].start()
    for i, cluster in enumerate(clusters):
        queue.put((i, cluster2file[cluster]))
    for i in processes:
        queue.put((None, None))
    queue.close()
    queue.join_thread()
    for process in processes:
        process.join()
        process.close()
    for cluster, n50 in zip(clusters, n50s):
        cluster2n50[cluster] = n50
    return cluster2n50


def computeClusterScore(clusters, cluster2completenessContamination, cluster2n50):
    '''
    An expanded reference map of the human gut microbiome reveals hundreds of previously unknown species
    '''
    cluster2score = dict()
    for cluster in clusters:
        completeness, contamination = cluster2completenessContamination[cluster]
        cluster2score[cluster] = completeness - 5 * contamination + 15 * log10(cluster2n50[cluster])
    return cluster2score


def readMashFile(nodes, file):
    n = len(nodes)
    node2index = dict((node, i) for i, node in enumerate(nodes))
    matrix = numpy.zeros(shape = (n, n), dtype = numpy.int8)
    openFile = open(file, 'r')
    for line in openFile:
        lines = line.rstrip('\n').split('\t')
        if lines[0] != lines[1]:
            nodeI = node2index[lines[0]]
            nodeJ = node2index[lines[1]]
            matrix[nodeI, nodeJ] = 1
            matrix[nodeJ, nodeI] = 1
    openFile.close()
    return matrix


def determineRepresentivesMash(mash, mashP, mashD, mashK, mashS, cluster2file, dpcofg2s2x):
    representives = list()
    processBar = ProcessBar(len(dpcofg2s2x))
    X = list()
    for i, (dpcofg, s2x) in enumerate(dpcofg2s2x.items(), start = 1):
        for s, x in s2x.items():
            if s != 's__':
                X.append(max(x))
                representives.append((X[-1][1], cluster2file[X[-1][1]], f'{dpcofg};{s}'))
        if 's__' in s2x:
            offset = len(X)
            s2x['s__'].sort(reverse = True)
            X.extend(s2x['s__'])
            clusterFiles = [cluster2file[i[1]] for i in X]
            tempfile = runMash(mash, mashD, mashP, mashK, mashS, clusterFiles)
            matrix = readMashFile(clusterFiles, tempfile)
            os.remove(tempfile)
            y = numpy.zeros(shape = matrix.shape[0], dtype = numpy.bool_)
            y[ : offset] = True
            j = 1
            for k in range(offset, y.size):
                if numpy.all(matrix[k, y] == 0):
                    y[k] = True
                    g = dpcofg.rsplit(';', maxsplit = 1)[1][3 : ]
                    representives.append((X[k][1], cluster2file[X[k][1]], f'{dpcofg};s__{g} {j}'))
                    j += 1
        X.clear()
        processBar.plot(i)
    return representives


def readFastaniFile(nodes, ani, file):
    n = len(nodes)
    node2index = dict((node, i) for i, node in enumerate(nodes))
    matrix = numpy.zeros(shape = (n, n), dtype = numpy.int8)
    openFile = open(file, 'r')
    for line in openFile:
        lines = line.rstrip('\n').split('\t')
        if lines[0] != lines[1] and float(lines[2]) >= ani:
            nodeI = node2index[lines[0]]
            nodeJ = node2index[lines[1]]
            matrix[nodeI, nodeJ] = 1
            matrix[nodeJ, nodeI] = 1
    openFile.close()
    return matrix


def determineRepresentivesFastani(fastani, fastaniT, fastaniANI, fastaniK, fastaniFragLen, cluster2file, dpcofg2s2x):
    representives = list()
    processBar = ProcessBar(len(dpcofg2s2x))
    X = list()
    for i, (dpcofg, s2x) in enumerate(dpcofg2s2x.items(), start = 1):
        for s, x in s2x.items():
            if s != 's__':
                X.append(max(x))
                representives.append((X[-1][1], cluster2file[X[-1][1]], f'{dpcofg};{s}'))
        if 's__' in s2x:
            offset = len(X)
            s2x['s__'].sort(reverse = True)
            X.extend(s2x['s__'])
            clusterFiles = [cluster2file[i[1]] for i in X]
            tempfile = runFastani(fastani, fastaniT, fastaniK, fastaniFragLen, clusterFiles)
            matrix = readFastaniFile(clusterFiles, fastaniANI, tempfile)
            os.remove(tempfile)
            y = numpy.zeros(shape = matrix.shape[0], dtype = numpy.bool_)
            y[ : offset] = True
            j = 1
            for k in range(offset, y.size):
                if numpy.all(matrix[k, y] == 0):
                    y[k] = True
                    g = dpcofg.rsplit(';', maxsplit = 1)[1][3 : ]
                    representives.append((X[k][1], cluster2file[X[k][1]], f'{dpcofg};s__{g} {j}'))
                    j += 1
        X.clear()
        processBar.plot(i)
    return representives


def writeFile(prefix, x):
    openFile1 = open(f'{prefix}.annotation', 'w')
    openFile1.write('Cluster ID\tClassification\n')
    openFile2 = open(f'{prefix}.mapping', 'w')
    openFile2.write('Sequence ID\tCluster ID\n')
    openFile3 = open(f'{prefix}.assembly', 'w')
    for cluster, file, classification in x:
        openFile1.write(f'{cluster}\t{classification}\n')
        for sequenceID, sequence in readFastaFile(file):
            openFile2.write(f'{cluster}-{sequenceID}\t{cluster}\n')
            openFile3.write(f'>{cluster}-{sequenceID}\n')
            openFile3.write(sequence + '\n')
    openFile1.close()
    openFile2.close()
    openFile3.close()
    return None


def main(parameters):
    cluster2file = createMapping(parameters.fasta)
    print(f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")} -> Loading CheckM2 file.', flush = True)
    cluster2completenessContamination = readCheckm2File(parameters.checkm2, parameters.contamination, parameters.completeness)
    print(f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")} -> Loading GTDB-TK file.', flush = True)
    cluster2dpcofg_s = readGtdbtkFile(parameters.gtdbtk)

    clusters = sorted(set(cluster2file.keys()) & set(cluster2completenessContamination.keys()) & set(cluster2dpcofg_s.keys()))

    print(f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")} -> Computing cluster n50.', flush = True)
    cluster2n50 = computeN50(clusters, cluster2file, parameters.threads)
    print(f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")} -> Computing cluster score.', flush = True)
    cluster2score = computeClusterScore(clusters, cluster2completenessContamination, cluster2n50)
    print(f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")} -> Determining representives.', flush = True)
    dpcofg2s2x = dict()
    for cluster in clusters:
        dpcofg_s = cluster2dpcofg_s[cluster]
        dpcofg2s2x.setdefault(dpcofg_s[0], dict()).setdefault(dpcofg_s[1], list()).append((cluster2score[cluster], cluster))
    if parameters.engine == 'mash':
        representives = determineRepresentivesMash(
            parameters.mash, parameters.threads, parameters.mash_distance, parameters.mash_kmer_size, parameters.mash_sketch_size, cluster2file, dpcofg2s2x
        )
    else:
        representives = determineRepresentivesFastani(
            parameters.fastani, parameters.threads, parameters.fastani_ani, parameters.fastani_kmer_size, parameters.fastani_fragment_length, cluster2file, dpcofg2s2x
        )
    print(f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")} -> Writing to \"{parameters.output}.*\".', flush = True)
    writeFile(parameters.output, representives)
    print(f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")} -> Finished.', flush = True)
    return None
