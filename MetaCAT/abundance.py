import os
import gzip
from ctypes import c_float, c_int64
from datetime import datetime
from multiprocessing import Process, Queue
from multiprocessing.sharedctypes import RawArray, Value

import numpy
from threadpoolctl import threadpool_limits

from .bam import getUngappedRegions, indexBam, readIndex
from .processbar import ProcessBar


def isGzipped(file):
    openFile = open(file, 'rb')
    magicCode = openFile.read(2)
    openFile.close()
    return magicCode == b'\x1f\x8b'


def readAnnotationFile(file):
    tree = set()
    cluster2nodes = dict()
    temp = list()
    if isGzipped(file):
        openFile = gzip.open(file, mode = 'rt')
    else:
        openFile = open(file, 'r')
    header = openFile.readline()
    assert header.startswith('Cluster ID\tClassification'), f'\"{file}\" is not a valid output generated by MetaCAT\'s representative.'
    for line in openFile:
        lines = line.rstrip('\n').split('\t', maxsplit = 2)
        if not lines[1].startswith('Unclassified'):
            nodes = cluster2nodes.setdefault(lines[0], list())
            # d__;p__;c__;o__;f__;g__;s__ #
            for i in lines[1].split(';'):
                if len(i) > 3:
                    temp.append(i)
                    node = ';'.join(temp)
                    tree.add(node)
                    nodes.append(node)
                else:
                    break
            temp.clear()
    openFile.close()

    tree = sorted(tree)
    node2index = dict((node, i) for i, node in enumerate(tree))
    for nodes in cluster2nodes.values():
        for i, node in enumerate(nodes):
            nodes[i] = node2index[node]
    return (tree, cluster2nodes)


def readMappingFile(file, clusters):
    Clusters = list()
    sequence2cluster = dict()
    cluster2index = dict()
    index = 0
    openFile = open(file, 'r')
    assert openFile.readline().rstrip('\n') == 'Sequence ID\tCluster ID', f'\"{file}\" must have a single header line: Sequence ID<tab>Cluster ID.'
    for line in openFile:
        lines = line.rstrip('\n').split('\t')
        if lines[1] in clusters:
            if lines[1] not in cluster2index:
                Clusters.append(lines[1])
                cluster2index[lines[1]] = index
                index += 1
            sequence2cluster[lines[0]] = cluster2index[lines[1]]
    openFile.close()
    return (Clusters, sequence2cluster)


def abundanceProcess(queue, x, n, m, cluster2nodes, trim, mapq, identity, abundance, M, processBar):
    x = numpy.ndarray(shape = (n, m), dtype = numpy.float32, buffer = x)
    while True:
        i, bamFile, mappingFile = queue.get()
        if i is None:
            break
        unmappedReads = 0
        clusters, sequence2cluster = readMappingFile(mappingFile, cluster2nodes)
        y = numpy.zeros(shape = (len(clusters), 3), dtype = numpy.float32) # depth, length, mappedReads #
        marker, sequences, lengths, fileOffsets, dataOffsets, dataSizes = readIndex(f'{bamFile}.index')
        for sequence, length, fileOffset, dataOffset, dataSize in zip(sequences, lengths, fileOffsets, dataOffsets, dataSizes):
            if (cluster := sequence2cluster.get(sequence, -1)) >= 0:
                minPosition = trim
                maxPosition = length - trim
                y[cluster, 1] += max(0, length - 2 * trim)
                for alignmentType, regions in getUngappedRegions(bamFile, fileOffset, dataOffset, dataSize, mapq, identity):
                    # alignmentType: 0 - unmapped alignment, 1 - mapped alignment #
                    if minPosition < maxPosition:
                        if alignmentType:
                            y[cluster, 2] += 1
                            for (regionStart, regionEnd) in regions:
                                y[cluster, 0] += max(0, min(regionEnd, maxPosition) - max(regionStart, minPosition))
                        else:
                            unmappedReads += 1
                    elif alignmentType:
                        y[cluster, 2] += 1
                    else:
                        unmappedReads += 1
            else:
                for alignmentType, regions in getUngappedRegions(bamFile, fileOffset, dataOffset, dataSize, mapq, identity):
                    # alignmentType: 0 - unmapped alignment, 1 - mapped alignment #
                    unmappedReads += 1
        y[ : , 1] += 10 * numpy.finfo(numpy.float32).eps
        y[ : , 0] /= y[ : , 1]
        mask = y[ : , 0] < abundance
        y[mask, 0] = 0
        mappedReads = numpy.sum(y[~mask, 2], dtype = numpy.float32)
        unmappedReads += numpy.sum(y[mask, 2], dtype = numpy.float32)
        y[ : , 0] *= mappedReads / ((mappedReads + unmappedReads + 1e-10) * (numpy.sum(y[ : , 0]) + 10 * numpy.finfo(numpy.float32).eps))
        for cluster, j in zip(clusters, y[ : , 0]):
            x[cluster2nodes[cluster], i] += j
        M.acquire()
        M.value += 1
        processBar.plot(M.value)
        M.release()
    return None


def createProcesses(cluster2nodes, n, m, trim, mapq, identity, abundance, threads):
    sharedX = RawArray(c_float, n * m)
    M = Value(c_int64, 0)
    x = numpy.ndarray(shape = (n, m), dtype = numpy.float32, buffer = sharedX)
    queue = Queue(threads)
    processes = list()
    processBar = ProcessBar(m)
    with threadpool_limits(limits = 1):
        for i in range(threads):
            processes.append(Process(target = abundanceProcess, args = (queue, sharedX, n, m, cluster2nodes, trim, mapq, identity, abundance, M, processBar)))
            processes[-1].start()
    return (queue, processes, x, M)


def freeProcesses(queue, processes):
    for process in processes:
        queue.put((None, None, None))
    queue.close()
    queue.join_thread()
    for process in processes:
        process.join()
        process.close()
    return None


def writeFile(file, x, rowNames, columnNames):
    openFile = open(file, 'w')
    openFile.write('\t'.join(['Abundance', ] + columnNames) + '\n')
    for rowName, x_ in zip(rowNames, x):
        openFile.write(rowName + '\t' + '\t'.join(x_.astype(numpy.str_).tolist()) + '\n')
    openFile.close()
    return None


def main(parameters):
    print(f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")} -> Indexing all bam files.', flush = True)
    indexBam(parameters.bam, parameters.threads_index)

    print(f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")} -> Loading annotation file.', flush = True)
    tree, cluster2nodes = readAnnotationFile(parameters.annotation)

    print(f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")} -> Computing abundance.', flush = True)
    n = len(tree)
    m = len(parameters.bam)
    queue, processes, x, M = createProcesses(cluster2nodes, n, m, parameters.trim, parameters.mapq, parameters.identity, parameters.min_abundance, parameters.threads_count)
    if len(parameters.mapping) == 1:
        parameters.mapping *= m
    for i, (bamFile, mappingFile) in enumerate(zip(parameters.bam, parameters.mapping)):
        queue.put((i, bamFile, mappingFile))
    freeProcesses(queue, processes)
    writeFile(parameters.output, x, tree, [os.path.splitext(os.path.basename(i))[0] for i in parameters.bam])
    print(f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")} -> Finished.', flush = True)
    return None
