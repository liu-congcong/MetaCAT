import gzip
import os
from ctypes import c_float, c_int64
from datetime import datetime
from multiprocessing import Process, Queue
from multiprocessing.sharedctypes import RawArray, Value

import numpy
from threadpoolctl import threadpool_limits

from .bam import getUngappedRegions, indexBam, readIndex
from .processbar import ProcessBar


def isGzipped(file):
    openFile = open(file, 'rb')
    magicCode = openFile.read(2)
    openFile.close()
    return magicCode == b'\x1f\x8b'


def readAnnotationFile(file):
    tree = set()
    cluster2nodes = dict()
    temp = list()
    if isGzipped(file):
        openFile = gzip.open(file, mode = 'rt')
    else:
        openFile = open(file, 'r')
    header = openFile.readline()
    assert header.startswith('Cluster ID\tClassification'), f'\"{file}\" is not a valid output generated by MetaCAT\'s representative.'
    for line in openFile:
        lines = line.rstrip('\n').split('\t', maxsplit = 2)
        nodes = cluster2nodes.setdefault(lines[0], list())
        # d__;p__;c__;o__;f__;g__;s__ #
        for i in lines[1].split(';'):
            temp.append(i)
            node = ';'.join(temp)
            tree.add(node)
            nodes.append(node)
        temp.clear()
    openFile.close()

    tree = sorted(tree)
    node2index = dict((node, i) for i, node in enumerate(tree))
    for nodes in cluster2nodes.values():
        for i, node in enumerate(nodes):
            nodes[i] = node2index[node]
    return (tree, cluster2nodes)


def readMappingFile(file):
    clusters = list()
    sequence2i = dict()
    cluster2index = dict()
    index = 0
    openFile = open(file, 'r')
    assert openFile.readline().rstrip('\n') == 'Sequence ID\tCluster ID', f'\"{file}\" must have a single header line: Sequence ID<tab>Cluster ID.'
    for line in openFile:
        lines = line.rstrip('\n').split('\t')
        if lines[1] not in cluster2index:
            clusters.append(lines[1])
            cluster2index[lines[1]] = index
            index += 1
        sequence2i[lines[0]] = cluster2index[lines[1]]
    openFile.close()
    return (clusters, sequence2i)


@threadpool_limits.wrap(limits = 1)
def abundanceProcess(queue, x, n, m, clusters, cluster2nodes, sequence2i, trim, mapq, identity, abundance, M):
    processBar = ProcessBar(m)
    x = numpy.ndarray(shape = (n, m), dtype = numpy.float32, buffer = x)
    while True:
        i, file = queue.get()
        if i is None:
            break
        unmappedReads = 0
        y = numpy.zeros(shape = (len(clusters), 3), dtype = numpy.float32) # depth, length, mappedReads #
        marker, headerFileOffset, headerDataOffset, sequences, lengths, fileOffsets, dataOffsets, dataSizes = readIndex(f'{file}.index')
        for sequence, length in zip(sequences, lengths):
            if (cluster := sequence2i.get(sequence, -1)) >= 0:
                y[cluster, 1] += max(0, length - 2 * trim)
        j0 = -2
        for j in getUngappedRegions(file, headerFileOffset, headerDataOffset, sum(dataSizes), mapq, identity):
            if (cluster := sequence2i.get(sequences[j[0]], -1)) >= 0:
                if j[0] != j0:
                    minPosition = trim
                    maxPosition = lengths[j[0]] - trim
                    j0 = j[0]
                if minPosition < maxPosition:
                    if j[1]:
                        y[cluster, 2] += 1
                        for (regionStart, regionEnd) in j[2]:
                            y[cluster, 0] += max(0, min(regionEnd, maxPosition) - max(regionStart, minPosition))
                    else:
                        unmappedReads += 1
                elif j[1]:
                    y[cluster, 2] += 1
                else:
                    unmappedReads += 1
            else:
                unmappedReads += 1
        y[ : , 1] += 10 * numpy.finfo(numpy.float32).eps
        y[ : , 0] /= y[ : , 1]
        mask = y[ : , 0] < abundance
        y[mask, 0] = 0
        mappedReads = numpy.sum(y[~mask, 2], dtype = numpy.float32)
        unmappedReads += numpy.sum(y[mask, 2], dtype = numpy.float32)
        y[ : , 0] *= mappedReads / ((mappedReads + unmappedReads + 10 * numpy.finfo(numpy.float32).eps) * (numpy.sum(y[ : , 0]) + 10 * numpy.finfo(numpy.float32).eps))
        for cluster, j in zip(clusters, y[ : , 0]):
            x[cluster2nodes[cluster], i] += j
        M.acquire()
        M.value += 1
        processBar.plot(M.value)
        M.release()
    return None


def createProcesses(clusters, cluster2nodes, sequence2i, n, m, trim, mapq, identity, abundance, threads):
    sharedX = RawArray(c_float, n * m)
    M = Value(c_int64, 0)
    x = numpy.ndarray(shape = (n, m), dtype = numpy.float32, buffer = sharedX)
    queue = Queue(threads)
    processes = list()
    for i in range(threads):
        processes.append(Process(target = abundanceProcess, args = (queue, sharedX, n, m, clusters, cluster2nodes, sequence2i, trim, mapq, identity, abundance, M)))
        processes[-1].start()
    return (queue, processes, x, M)


def freeProcesses(queue, processes):
    for process in processes:
        queue.put((None, None))
    queue.close()
    queue.join_thread()
    for process in processes:
        process.join()
        process.close()
    return None


def writeFile(file, x, rowNames, columnNames):
    openFile = open(file, 'w')
    openFile.write(columnNames + '\n')
    for rowName, y in zip(rowNames, x):
        z = '\t'.join(str(i) for i in y)
        openFile.write(f'{rowName}\t{z}\n')
    openFile.close()
    return None


def main(parameters):
    print(f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")} -> Indexing all bam files.', flush = True)
    indexBam(parameters.bam, parameters.threads_index)

    print(f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")} -> Loading annotation file.', flush = True)
    tree, cluster2nodes = readAnnotationFile(parameters.annotation)

    print(f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")} -> Loading mapping file.', flush = True)
    clusters, sequence2i = readMappingFile(parameters.mapping)

    print(f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")} -> Computing abundance.', flush = True)
    n = len(tree)
    m = len(parameters.bam)
    queue, processes, x, M = createProcesses(clusters, cluster2nodes, sequence2i, n, m, parameters.trim, parameters.mapq, parameters.identity, parameters.min_abundance, parameters.threads_count)
    for i, bamFile in enumerate(parameters.bam):
        queue.put((i, bamFile))
    freeProcesses(queue, processes)
    individuals = '\t'.join(os.path.splitext(os.path.basename(i))[0] for i in parameters.bam)
    writeFile(parameters.output, x, tree, f'Abundance\t{individuals}')
    print(f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")} -> Finished.', flush = True)
    return None
